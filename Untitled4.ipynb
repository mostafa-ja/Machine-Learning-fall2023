{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/Machine-Learning-fall2023/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Questions"
      ],
      "metadata": {
        "id": "1xDv7Luc5fw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps to Build an Image Classification Model using CNN"
      ],
      "metadata": {
        "id": "EcsYA8FS119I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we train a CNN model, let’s build a basic, Fully Connected Neural Network for the dataset. The basic steps to build an image classification model using a neural network are:\n",
        "\n",
        "1. Flatten the input image dimensions to 1D (width pixels x height pixels)\n",
        "2. Normalize the image pixel values (divide by 255)\n",
        "3. One-Hot Encode the categorical column\n",
        "4. Build a model architecture (Sequential) with Dense layers(Fully connected layers)\n",
        "5. Train the model and make predictions\n",
        "\n",
        "Here’s how you can build a neural network model for MNIST. I have used relu and softmax as the activation function and adam optimizer, with accuracy being the evaluation metrics. The code contains all the steps from data loading to preprocessing to fitting the model. I have commented on the relevant parts of the code for better understanding:"
      ],
      "metadata": {
        "id": "kI77A2M92ANe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# keras imports for the dataset and building our neural network\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D\n",
        "from keras.utils import to_categorical\n",
        "#from keras.utils import np_utils\n"
      ],
      "metadata": {
        "id": "yovPUtV22WNW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Flattening the images from the 28x28 pixels to 1D 787 pixels\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# normalizing the data to help with the training\n",
        "X_train /= 255.\n",
        "X_test /= 255.\n",
        "\n",
        "# one-hot encoding using keras' numpy-related utilities\n",
        "n_classes = 10\n",
        "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
        "Y_train = to_categorical(y_train, n_classes)\n",
        "Y_test = to_categorical(y_test, n_classes)\n",
        "print(\"Shape after one-hot encoding: \", Y_train.shape)\n"
      ],
      "metadata": {
        "id": "QcRo7d-J1__P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c078a322-f39f-450c-e82c-e1add6422b43"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Shape before one-hot encoding:  (60000,)\n",
            "Shape after one-hot encoding:  (60000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ne6Y8K9q11Zu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3c0f80e-f3df-4958-db02-844d7536fcfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (None, 100)               78500     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 79510 (310.59 KB)\n",
            "Trainable params: 79510 (310.59 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "235/235 [==============================] - 2s 6ms/step - loss: 0.4667 - accuracy: 0.8749 - val_loss: 0.2462 - val_accuracy: 0.9313\n",
            "Epoch 2/20\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2192 - accuracy: 0.9385 - val_loss: 0.1919 - val_accuracy: 0.9446\n",
            "Epoch 3/20\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1667 - accuracy: 0.9521 - val_loss: 0.1537 - val_accuracy: 0.9553\n",
            "Epoch 4/20\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1344 - accuracy: 0.9617 - val_loss: 0.1283 - val_accuracy: 0.9618\n",
            "Epoch 5/20\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1133 - accuracy: 0.9675 - val_loss: 0.1179 - val_accuracy: 0.9651\n",
            "Epoch 6/20\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0958 - accuracy: 0.9723 - val_loss: 0.1046 - val_accuracy: 0.9692\n",
            "Epoch 7/20\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0836 - accuracy: 0.9762 - val_loss: 0.0985 - val_accuracy: 0.9691\n",
            "Epoch 8/20\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.0734 - accuracy: 0.9796 - val_loss: 0.0920 - val_accuracy: 0.9724\n",
            "Epoch 9/20\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0645 - accuracy: 0.9821 - val_loss: 0.0870 - val_accuracy: 0.9742\n",
            "Epoch 10/20\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0580 - accuracy: 0.9840 - val_loss: 0.0814 - val_accuracy: 0.9747\n",
            "Epoch 11/20\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0516 - accuracy: 0.9859 - val_loss: 0.0801 - val_accuracy: 0.9753\n",
            "Epoch 12/20\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0466 - accuracy: 0.9876 - val_loss: 0.0757 - val_accuracy: 0.9776\n",
            "Epoch 13/20\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0419 - accuracy: 0.9887 - val_loss: 0.0763 - val_accuracy: 0.9772\n",
            "Epoch 14/20\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0372 - accuracy: 0.9905 - val_loss: 0.0796 - val_accuracy: 0.9766\n",
            "Epoch 15/20\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0341 - accuracy: 0.9912 - val_loss: 0.0742 - val_accuracy: 0.9764\n",
            "Epoch 16/20\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0301 - accuracy: 0.9924 - val_loss: 0.0749 - val_accuracy: 0.9769\n",
            "Epoch 17/20\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0271 - accuracy: 0.9936 - val_loss: 0.0744 - val_accuracy: 0.9767\n",
            "Epoch 18/20\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.0243 - accuracy: 0.9944 - val_loss: 0.0757 - val_accuracy: 0.9777\n",
            "Epoch 19/20\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.0223 - accuracy: 0.9947 - val_loss: 0.0740 - val_accuracy: 0.9773\n",
            "Epoch 20/20\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0199 - accuracy: 0.9956 - val_loss: 0.0707 - val_accuracy: 0.9793\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c1615c0b550>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# building a linear stack of layers with the sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# hidden layer\n",
        "\n",
        "model.add(Dense(100, input_shape=(784,), activation='relu'))\n",
        "\n",
        "# output layer\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "# looking at the model summary\n",
        "model.summary()\n",
        "# compiling the sequential model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "\n",
        "\n",
        "# training the model for 10 epochs (use model.fit function)\n",
        "model.fit(X_train, Y_train, batch_size=256, epochs=20, validation_data=(X_test, Y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running the above code, you’d realized that we are getting a good validation accuracy of around 97% easily.\n",
        "\n",
        "One major advantage of using ConvNets over NNs is that you do not need to flatten the input images to 1D as they are capable of working with image data in 2D. This helps in retaining the “spatial” properties of images."
      ],
      "metadata": {
        "id": "4ZA86kTT22-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Code for the CNN Model\n"
      ],
      "metadata": {
        "id": "KU9pJrK82_ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# keras imports for the dataset and building our neural network\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
        "#from keras.utils import np_utils\n",
        "\n",
        "# to calculate accuracy\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "metadata": {
        "id": "CC-6DWpR3GVN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# building the input vector from the 28x28 pixels\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# normalizing the data to help with the training\n",
        "X_train /= 255.\n",
        "X_test /= 255.\n",
        "\n",
        "# one-hot encoding using keras' numpy-related utilities\n",
        "n_classes = 10\n",
        "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
        "Y_train = to_categorical(y_train, n_classes)\n",
        "Y_test = to_categorical(y_test, n_classes)\n",
        "print(\"Shape after one-hot encoding: \", Y_train.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "V4k7JpTd3ESt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34d2e6d9-ebf8-4e94-b407-36f793d18ac3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before one-hot encoding:  (60000,)\n",
            "Shape after one-hot encoding:  (60000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# building a linear stack of layers with the sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# convolutional layer\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu', input_shape=(28,28,1)))\n",
        "\n",
        "\n",
        "# MaxPool layer\n",
        "\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "# flatten output of conv\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "# hidden layer\n",
        "\n",
        "model.add(Dense(100, activation='relu'))\n",
        "\n",
        "# output layer\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "# compiling the sequential model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "\n",
        "# training the model for 10 epochs\n",
        "model.fit(X_train, Y_train, batch_size=256, epochs=10, validation_data=(X_test, Y_test))"
      ],
      "metadata": {
        "id": "JWj4yk1R3OG2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87fe8ba5-6891-45a6-824f-139d1e6eb658"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "235/235 [==============================] - 26s 109ms/step - loss: 0.3214 - accuracy: 0.9096 - val_loss: 0.1255 - val_accuracy: 0.9642\n",
            "Epoch 2/10\n",
            "235/235 [==============================] - 22s 92ms/step - loss: 0.1034 - accuracy: 0.9701 - val_loss: 0.0824 - val_accuracy: 0.9744\n",
            "Epoch 3/10\n",
            "235/235 [==============================] - 24s 101ms/step - loss: 0.0676 - accuracy: 0.9806 - val_loss: 0.0634 - val_accuracy: 0.9805\n",
            "Epoch 4/10\n",
            "235/235 [==============================] - 24s 100ms/step - loss: 0.0491 - accuracy: 0.9860 - val_loss: 0.0516 - val_accuracy: 0.9836\n",
            "Epoch 5/10\n",
            "235/235 [==============================] - 22s 94ms/step - loss: 0.0393 - accuracy: 0.9887 - val_loss: 0.0497 - val_accuracy: 0.9839\n",
            "Epoch 6/10\n",
            "235/235 [==============================] - 22s 95ms/step - loss: 0.0320 - accuracy: 0.9910 - val_loss: 0.0496 - val_accuracy: 0.9837\n",
            "Epoch 7/10\n",
            "235/235 [==============================] - 23s 99ms/step - loss: 0.0274 - accuracy: 0.9920 - val_loss: 0.0493 - val_accuracy: 0.9846\n",
            "Epoch 8/10\n",
            "235/235 [==============================] - 21s 91ms/step - loss: 0.0216 - accuracy: 0.9938 - val_loss: 0.0442 - val_accuracy: 0.9856\n",
            "Epoch 9/10\n",
            "235/235 [==============================] - 25s 105ms/step - loss: 0.0177 - accuracy: 0.9949 - val_loss: 0.0470 - val_accuracy: 0.9848\n",
            "Epoch 10/10\n",
            "235/235 [==============================] - 27s 113ms/step - loss: 0.0151 - accuracy: 0.9956 - val_loss: 0.0484 - val_accuracy: 0.9851\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c161d35de70>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upZqhxR5aYvQ",
        "outputId": "889358e2-06cf-4d8f-baee-a1a4b7a2723f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 13, 13, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 5408)              0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 100)               540900    \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 542230 (2.07 MB)\n",
            "Trainable params: 542230 (2.07 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though our max validation accuracy by using a simple neural network model was around 97%, the CNN model is able to get 98%+ with just a single convolution layer! :)"
      ],
      "metadata": {
        "id": "d9e7-OIL3nOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanatory Questions\n"
      ],
      "metadata": {
        "id": "v3e20ACH5aHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Q1. explain about each activation Functions used in Neural Networks and compare them:\n",
        "\n",
        "a.Sigmoid\n",
        "\n",
        "b.Tanh\n",
        "\n",
        "c.Relu\n",
        "\n",
        "\n",
        "\n",
        "### a. Sigmoid Activation Function:\n",
        "\n",
        "The sigmoid function, also known as the logistic function, squashes its input to a range between 0 and 1. The mathematical form of the sigmoid function is given by:\n",
        "\n",
        "sigmoid(x) = 1 / (1 + e^(-x))\n",
        "\n",
        "**Characteristics:**\n",
        "- Output range: (0, 1)\n",
        "- Smooth, differentiable, and monotonic.\n",
        "- Suitable for binary classification problems, as it can be interpreted as a probability.\n",
        "- Prone to vanishing gradient problem, especially in deep networks, which can slow down learning.\n",
        "\n",
        "### b. Tanh Activation Function:\n",
        "\n",
        "The hyperbolic tangent (tanh) function is similar to the sigmoid but maps its input to a range between -1 and 1. The mathematical form of the tanh function is given by:\n",
        "\n",
        "tanh(x) = (e^(x) - e^(-x)) / (e^(x) + e^(-x))\n",
        "\n",
        "**Characteristics:**\n",
        "- Output range: (-1, 1)\n",
        "- Zero-centered, which helps mitigate the vanishing gradient problem compared to sigmoid.\n",
        "- Suffers from vanishing gradient problem for deep networks, though less severe than sigmoid.\n",
        "- Commonly used in hidden layers of neural networks.\n",
        "\n",
        "### c. ReLU (Rectified Linear Unit) Activation Function:\n",
        "\n",
        "ReLU is a popular activation function that introduces non-linearity by outputting the input for positive values and zero for negative values. The mathematical form of the ReLU function is given by:\n",
        "\n",
        "ReLU(x) = max(0, x)\n",
        "\n",
        "**Characteristics:**\n",
        "- Simple and computationally efficient.\n",
        "- Addresses the vanishing gradient problem for positive values.\n",
        "- Prone to the \"dying ReLU\" problem, where neurons can become inactive during training and stop learning.\n",
        "- Commonly used in hidden layers but may not be suitable for all types of networks.\n",
        "\n",
        "### Comparison:\n",
        "\n",
        "1. **Output Range:**\n",
        "   - Sigmoid: (0, 1)\n",
        "   - Tanh: (-1, 1)\n",
        "   - ReLU: [0, +∞)\n",
        "\n",
        "2. **Vanishing Gradient:**\n",
        "   - Sigmoid and tanh are prone to the vanishing gradient problem, especially in deep networks.\n",
        "   - ReLU helps mitigate the vanishing gradient problem for positive values.\n",
        "\n",
        "3. **Zero-Centered:**\n",
        "   - Sigmoid and ReLU are not zero-centered.\n",
        "   - Tanh is zero-centered, which can help with optimization.\n",
        "\n",
        "4. **Common Usage:**\n",
        "   - Sigmoid: Typically used in the output layer for binary classification.\n",
        "   - Tanh: Commonly used in hidden layers.\n",
        "   - ReLU: Popular in hidden layers.\n"
      ],
      "metadata": {
        "id": "cn_AxHiH4Nvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2. explain about Epoch, Batch, and Iteration in Neural Networks\n",
        "\n",
        "**Epoch**: One complete pass through the entire training dataset.\n",
        "\n",
        "**Batch**: A subset of the training dataset, used for updating the model parameters.\n",
        "\n",
        "**Iteration**: One update of the model's weights, typically after processing one batch."
      ],
      "metadata": {
        "id": "Ezkbj4Wm4z1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3.  What is the difference between a convolutional neural network and a fully connected neural network?\n",
        "\n",
        "\n",
        "\n",
        "1. **Local Connectivity vs. Global Connectivity:**\n",
        "   - **CNN:** CNNs are designed to exploit the spatial locality of patterns in input data. They use convolutional layers with filters (kernels) that slide over the input to capture local features. This allows CNNs to learn hierarchical representations of spatial patterns, making them well-suited for tasks like image recognition.\n",
        "   - **FCN:** In contrast, fully connected networks connect every neuron in one layer to every neuron in the next layer. This global connectivity is effective for learning relationships between different features, but it doesn't explicitly capture spatial structure in the input data.\n",
        "\n",
        "2. **Parameter Sharing:**\n",
        "   - **CNN:** CNNs use parameter sharing through the use of filters. The same filter is applied across different spatial locations, allowing the network to learn shared patterns. This significantly reduces the number of parameters compared to a fully connected network, making CNNs computationally more efficient.\n",
        "   - **FCN:** In fully connected networks, each connection between neurons has its own set of parameters. This can result in a large number of parameters, especially when dealing with high-dimensional input data.\n",
        "\n",
        "3. **Translation Invariance:**\n",
        "   - **CNN:** CNNs inherently exhibit translation invariance because of the local connectivity and parameter sharing. This means that the network can recognize patterns regardless of their exact spatial position in the input.\n",
        "   - **FCN:** Fully connected networks lack translation invariance as they process the entire input globally. They may struggle to recognize patterns in different spatial locations without explicitly learning variations.\n",
        "\n",
        "4. **Input Size:**\n",
        "   - **CNN:** CNNs are well-suited for grid-like data, such as images, where spatial relationships are crucial. They can handle inputs of varying sizes, thanks to their parameter sharing and pooling layers.\n",
        "   - **FCN:** Fully connected networks are more flexible with input sizes but might struggle with high-dimensional grid-like data due to the vast number of parameters.\n",
        "\n",
        "5. **Use Cases:**\n",
        "   - **CNN:** Primarily used for computer vision tasks, such as image classification, object detection, and segmentation, where spatial hierarchies and local patterns are important.\n",
        "   - **FCN:** More general-purpose and used in a variety of tasks, including classification and regression problems. They are not specialized for grid-like data and may require feature engineering to handle structured inputs effectively.\n"
      ],
      "metadata": {
        "id": "v75tFc42462m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4.  What is the role of activation functions in neural networks?\n",
        "\n",
        "Activation functions play a crucial role in neural networks by introducing non-linearities to the model. The primary purpose of activation functions is to enable neural networks to learn complex patterns and relationships in the data. Without activation functions, a neural network would essentially be a linear model, and stacking multiple layers of linear transformations would not increase the model's capacity to capture non-linearities."
      ],
      "metadata": {
        "id": "ZNH2B76A5B_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5.  What is a dropout layer, and how is it used in neural networks?\n",
        "\n",
        "- With dropout, certain nodes are set to the value zero in a training run, i.e. removed from the network. Thus, they have no influence on the prediction and also in the backpropagation. Thus, a new, slightly modified network architecture is built in each run and the network learns to produce good predictions without certain inputs.\n",
        "\n",
        "- During Training: At each training iteration, randomly selected neurons are \"dropped out\" or deactivated. This means their outputs are set to zero. The random dropout is typically applied independently to each neuron with a specified probability, often referred to as the dropout rate .\n",
        "\n",
        "- During Testing/Prediction: In the testing phase, all neurons are used, but their outputs are scaled by the dropout rate to account for the fact that during training, only a fraction of neurons was active. This scaling is done to ensure that the expected total input to a neuron remains approximately the same during both training and testing."
      ],
      "metadata": {
        "id": "1xK95Vy05Jmu"
      }
    }
  ]
}