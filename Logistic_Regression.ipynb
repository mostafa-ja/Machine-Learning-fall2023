{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/Machine-Learning-fall2023/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdtGQFxlc0Uy"
      },
      "source": [
        "## Answer these questions about Classificaton\n",
        "1. Why would you want to use: <br />\n",
        "a. Ridge regression instead of plain linear regression (i.e., without any\n",
        "regularization)?<br />\n",
        "b. Lasso instead of ridge regression?<br />\n",
        "c. Elastic net instead of lasso regression?\n",
        "\n",
        "• Ridge regression is preferred over plain linear regression when dealing with overfitting issues or datasets with high-dimensional features. It balances the tradeoff between bias and variance, leading to more stable and generalized models.\n",
        "\n",
        "• Lasso Regression uses an ℓ1 penalty, which tends to push the weights down to exactly zero. This leads to sparse models, where all weights are zero except for the most important weights. This is a way to perform feature selection automatically, which is good if you suspect that only a few features actually matter. When you are not sure, you should prefer Ridge Regression.\n",
        "\n",
        "• Elastic Net is generally preferred over Lasso since Lasso may behave erratically in some cases (when several features are strongly correlated or when there are more features than training instances). However, it does add an extra hyperparameter to tune. If you just want Lasso without the erratic behavior, you can just use Elastic Net with an l1_ratio close to 1.\n",
        "\n",
        "\n",
        "\n",
        "2. Suppose you want to classify pictures as outdoor/indoor and\n",
        "daytime/nighttime. Should you implement two logistic regression\n",
        "classifiers or one softmax regression classifier?\n",
        "\n",
        "If you want to classify pictures as outdoor/indoor and daytime/nighttime, since these are not exclusive classes (i.e., all four combinations are possible) you should train two Logistic Regression classifiers."
      ],
      "id": "UdtGQFxlc0Uy"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "subject-vector",
      "metadata": {
        "id": "subject-vector"
      },
      "outputs": [],
      "source": [
        "# lets import the essential packages we need\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rational-guidance",
      "metadata": {
        "id": "rational-guidance"
      },
      "source": [
        "# 1. Sign Language Classification\n",
        "\n",
        "The American Sign Language MNIST Dataset we are going to use is obtained from [Kaggle](https://www.kaggle.com/datamunge/sign-language-mnist). This dataset is much like the original MNIST dataset. Each training and test case consists of a numerical label (0–25) with a one-to-one correspondence to the English alphabet (0 corresponds to A) and a grayscale 28x28 pixel image with values ranging from 0–255. However, there is no label correspondence to the letter J (9) and Z (25) due to the motion required to symbolize those letters. The number of testing and training cases in this dataset are much lower compared to the orginal MNIST dataset since there are only 27,455 training cases and 7,172 tests cases in this dataset.\n",
        "\n",
        "<img src=\"/content/datasets/sign-dataset/american_sign_language.PNG\" width=700 height=700 />\n",
        "\n",
        "In this problem we are going to implement a simple multi-class classification model to classify each image to its desired label. For downloading the dataset, use this [link](https://www.kaggle.com/datamunge/sign-language-mnist).\n",
        "\n",
        "**Note:** You may need to create a new account in Kaggle if you have not registered yet. Then put the downloaded directory in the `dataset/` folder with the name of : `sign-dataset/`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import kaggle.json\n",
        "import gdown\n",
        "download_link = 'https://drive.google.com/uc?id=18bN4Mot3QMrtnaU-DWEcxVYuH3b9Un3Q'\n",
        "output_filepath = '/content/'\n",
        "gdown.download(download_link, output_filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "wEoyUDxENYXf",
        "outputId": "34177f9c-32e0-4399-a4eb-a3dbcc5cc045"
      },
      "id": "wEoyUDxENYXf",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18bN4Mot3QMrtnaU-DWEcxVYuH3b9Un3Q\n",
            "To: /content/kaggle.json\n",
            "100%|██████████| 69.0/69.0 [00:00<00:00, 151kB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/kaggle.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#upload data from kaggle website\n",
        "\n",
        "# Install Kaggle API.\n",
        "!pip install kaggle\n",
        "# Run the following code to configure the path to “kaggle.json”\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content\"\n",
        "#download the dataset\n",
        "!kaggle datasets download -d datamunge/sign-language-mnist\n",
        "#unzip the dataset\n",
        "!mkdir -p /content/datasets/sign-dataset/\n",
        "!unzip '/content/sign-language-mnist.zip' -d \"/content/datasets/sign-dataset/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU0j2xBnKbdz",
        "outputId": "e297c43a-7b01-42d5-8567-facde0012762"
      },
      "id": "rU0j2xBnKbdz",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/kaggle.json'\n",
            "Downloading sign-language-mnist.zip to /content\n",
            " 88% 55.0M/62.6M [00:00<00:00, 155MB/s]\n",
            "100% 62.6M/62.6M [00:00<00:00, 146MB/s]\n",
            "Archive:  /content/sign-language-mnist.zip\n",
            "  inflating: /content/datasets/sign-dataset/amer_sign2.png  \n",
            "  inflating: /content/datasets/sign-dataset/amer_sign3.png  \n",
            "  inflating: /content/datasets/sign-dataset/american_sign_language.PNG  \n",
            "  inflating: /content/datasets/sign-dataset/sign_mnist_test.csv  \n",
            "  inflating: /content/datasets/sign-dataset/sign_mnist_test/sign_mnist_test.csv  \n",
            "  inflating: /content/datasets/sign-dataset/sign_mnist_train.csv  \n",
            "  inflating: /content/datasets/sign-dataset/sign_mnist_train/sign_mnist_train.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "substantial-forum",
      "metadata": {
        "id": "substantial-forum"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import string\n",
        "\n",
        "train_dataset = pd.read_csv('/content/datasets/sign-dataset/sign_mnist_train/sign_mnist_train.csv')\n",
        "test_dataset = pd.read_csv('/content/datasets/sign-dataset/sign_mnist_test/sign_mnist_test.csv')\n",
        "num_rows = train_dataset.shape[0]\n",
        "# To map each label number to its corresponding letter\n",
        "letters = dict(enumerate(string.ascii_uppercase))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "letters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bc-RHhJ_OQpo",
        "outputId": "45220a17-36f4-4668-ac53-49410b527dae"
      },
      "id": "Bc-RHhJ_OQpo",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'A',\n",
              " 1: 'B',\n",
              " 2: 'C',\n",
              " 3: 'D',\n",
              " 4: 'E',\n",
              " 5: 'F',\n",
              " 6: 'G',\n",
              " 7: 'H',\n",
              " 8: 'I',\n",
              " 9: 'J',\n",
              " 10: 'K',\n",
              " 11: 'L',\n",
              " 12: 'M',\n",
              " 13: 'N',\n",
              " 14: 'O',\n",
              " 15: 'P',\n",
              " 16: 'Q',\n",
              " 17: 'R',\n",
              " 18: 'S',\n",
              " 19: 'T',\n",
              " 20: 'U',\n",
              " 21: 'V',\n",
              " 22: 'W',\n",
              " 23: 'X',\n",
              " 24: 'Y',\n",
              " 25: 'Z'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_rows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfULIGg4ORKl",
        "outputId": "9ca3bfde-66ba-42cb-b9ca-b62f4c3786bc"
      },
      "id": "mfULIGg4ORKl",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27455"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "plain-crack",
      "metadata": {
        "id": "plain-crack"
      },
      "source": [
        "Now it is the time foe data exploration! The first few rows of the training datast are shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gorgeous-buffer",
      "metadata": {
        "id": "gorgeous-buffer",
        "outputId": "e2fc3d4e-f6a7-4030-887c-e6b772da5db1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "      <th>pixel784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>107</td>\n",
              "      <td>118</td>\n",
              "      <td>127</td>\n",
              "      <td>134</td>\n",
              "      <td>139</td>\n",
              "      <td>143</td>\n",
              "      <td>146</td>\n",
              "      <td>150</td>\n",
              "      <td>153</td>\n",
              "      <td>...</td>\n",
              "      <td>207</td>\n",
              "      <td>207</td>\n",
              "      <td>207</td>\n",
              "      <td>207</td>\n",
              "      <td>206</td>\n",
              "      <td>206</td>\n",
              "      <td>206</td>\n",
              "      <td>204</td>\n",
              "      <td>203</td>\n",
              "      <td>202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6</td>\n",
              "      <td>155</td>\n",
              "      <td>157</td>\n",
              "      <td>156</td>\n",
              "      <td>156</td>\n",
              "      <td>156</td>\n",
              "      <td>157</td>\n",
              "      <td>156</td>\n",
              "      <td>158</td>\n",
              "      <td>158</td>\n",
              "      <td>...</td>\n",
              "      <td>69</td>\n",
              "      <td>149</td>\n",
              "      <td>128</td>\n",
              "      <td>87</td>\n",
              "      <td>94</td>\n",
              "      <td>163</td>\n",
              "      <td>175</td>\n",
              "      <td>103</td>\n",
              "      <td>135</td>\n",
              "      <td>149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>187</td>\n",
              "      <td>188</td>\n",
              "      <td>188</td>\n",
              "      <td>187</td>\n",
              "      <td>187</td>\n",
              "      <td>186</td>\n",
              "      <td>187</td>\n",
              "      <td>188</td>\n",
              "      <td>187</td>\n",
              "      <td>...</td>\n",
              "      <td>202</td>\n",
              "      <td>201</td>\n",
              "      <td>200</td>\n",
              "      <td>199</td>\n",
              "      <td>198</td>\n",
              "      <td>199</td>\n",
              "      <td>198</td>\n",
              "      <td>195</td>\n",
              "      <td>194</td>\n",
              "      <td>195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "      <td>212</td>\n",
              "      <td>212</td>\n",
              "      <td>211</td>\n",
              "      <td>210</td>\n",
              "      <td>211</td>\n",
              "      <td>210</td>\n",
              "      <td>210</td>\n",
              "      <td>...</td>\n",
              "      <td>235</td>\n",
              "      <td>234</td>\n",
              "      <td>233</td>\n",
              "      <td>231</td>\n",
              "      <td>230</td>\n",
              "      <td>226</td>\n",
              "      <td>225</td>\n",
              "      <td>222</td>\n",
              "      <td>229</td>\n",
              "      <td>163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13</td>\n",
              "      <td>164</td>\n",
              "      <td>167</td>\n",
              "      <td>170</td>\n",
              "      <td>172</td>\n",
              "      <td>176</td>\n",
              "      <td>179</td>\n",
              "      <td>180</td>\n",
              "      <td>184</td>\n",
              "      <td>185</td>\n",
              "      <td>...</td>\n",
              "      <td>92</td>\n",
              "      <td>105</td>\n",
              "      <td>105</td>\n",
              "      <td>108</td>\n",
              "      <td>133</td>\n",
              "      <td>163</td>\n",
              "      <td>157</td>\n",
              "      <td>163</td>\n",
              "      <td>164</td>\n",
              "      <td>179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>16</td>\n",
              "      <td>161</td>\n",
              "      <td>168</td>\n",
              "      <td>172</td>\n",
              "      <td>173</td>\n",
              "      <td>178</td>\n",
              "      <td>184</td>\n",
              "      <td>189</td>\n",
              "      <td>193</td>\n",
              "      <td>196</td>\n",
              "      <td>...</td>\n",
              "      <td>76</td>\n",
              "      <td>74</td>\n",
              "      <td>68</td>\n",
              "      <td>62</td>\n",
              "      <td>53</td>\n",
              "      <td>55</td>\n",
              "      <td>48</td>\n",
              "      <td>238</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>8</td>\n",
              "      <td>134</td>\n",
              "      <td>134</td>\n",
              "      <td>135</td>\n",
              "      <td>135</td>\n",
              "      <td>136</td>\n",
              "      <td>137</td>\n",
              "      <td>137</td>\n",
              "      <td>138</td>\n",
              "      <td>138</td>\n",
              "      <td>...</td>\n",
              "      <td>109</td>\n",
              "      <td>102</td>\n",
              "      <td>91</td>\n",
              "      <td>65</td>\n",
              "      <td>138</td>\n",
              "      <td>189</td>\n",
              "      <td>179</td>\n",
              "      <td>181</td>\n",
              "      <td>181</td>\n",
              "      <td>179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>22</td>\n",
              "      <td>114</td>\n",
              "      <td>42</td>\n",
              "      <td>74</td>\n",
              "      <td>99</td>\n",
              "      <td>104</td>\n",
              "      <td>109</td>\n",
              "      <td>117</td>\n",
              "      <td>127</td>\n",
              "      <td>142</td>\n",
              "      <td>...</td>\n",
              "      <td>214</td>\n",
              "      <td>218</td>\n",
              "      <td>220</td>\n",
              "      <td>223</td>\n",
              "      <td>223</td>\n",
              "      <td>225</td>\n",
              "      <td>227</td>\n",
              "      <td>227</td>\n",
              "      <td>228</td>\n",
              "      <td>228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>169</td>\n",
              "      <td>174</td>\n",
              "      <td>176</td>\n",
              "      <td>180</td>\n",
              "      <td>183</td>\n",
              "      <td>185</td>\n",
              "      <td>187</td>\n",
              "      <td>188</td>\n",
              "      <td>190</td>\n",
              "      <td>...</td>\n",
              "      <td>119</td>\n",
              "      <td>118</td>\n",
              "      <td>123</td>\n",
              "      <td>120</td>\n",
              "      <td>118</td>\n",
              "      <td>114</td>\n",
              "      <td>94</td>\n",
              "      <td>74</td>\n",
              "      <td>61</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3</td>\n",
              "      <td>189</td>\n",
              "      <td>189</td>\n",
              "      <td>189</td>\n",
              "      <td>190</td>\n",
              "      <td>190</td>\n",
              "      <td>191</td>\n",
              "      <td>190</td>\n",
              "      <td>190</td>\n",
              "      <td>190</td>\n",
              "      <td>...</td>\n",
              "      <td>13</td>\n",
              "      <td>53</td>\n",
              "      <td>200</td>\n",
              "      <td>204</td>\n",
              "      <td>201</td>\n",
              "      <td>201</td>\n",
              "      <td>193</td>\n",
              "      <td>175</td>\n",
              "      <td>178</td>\n",
              "      <td>156</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
              "0      3     107     118     127     134     139     143     146     150   \n",
              "1      6     155     157     156     156     156     157     156     158   \n",
              "2      2     187     188     188     187     187     186     187     188   \n",
              "3      2     211     211     212     212     211     210     211     210   \n",
              "4     13     164     167     170     172     176     179     180     184   \n",
              "5     16     161     168     172     173     178     184     189     193   \n",
              "6      8     134     134     135     135     136     137     137     138   \n",
              "7     22     114      42      74      99     104     109     117     127   \n",
              "8      3     169     174     176     180     183     185     187     188   \n",
              "9      3     189     189     189     190     190     191     190     190   \n",
              "\n",
              "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
              "0     153  ...       207       207       207       207       206       206   \n",
              "1     158  ...        69       149       128        87        94       163   \n",
              "2     187  ...       202       201       200       199       198       199   \n",
              "3     210  ...       235       234       233       231       230       226   \n",
              "4     185  ...        92       105       105       108       133       163   \n",
              "5     196  ...        76        74        68        62        53        55   \n",
              "6     138  ...       109       102        91        65       138       189   \n",
              "7     142  ...       214       218       220       223       223       225   \n",
              "8     190  ...       119       118       123       120       118       114   \n",
              "9     190  ...        13        53       200       204       201       201   \n",
              "\n",
              "   pixel781  pixel782  pixel783  pixel784  \n",
              "0       206       204       203       202  \n",
              "1       175       103       135       149  \n",
              "2       198       195       194       195  \n",
              "3       225       222       229       163  \n",
              "4       157       163       164       179  \n",
              "5        48       238       255       255  \n",
              "6       179       181       181       179  \n",
              "7       227       227       228       228  \n",
              "8        94        74        61        57  \n",
              "9       193       175       178       156  \n",
              "\n",
              "[10 rows x 785 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fitted-topic",
      "metadata": {
        "id": "fitted-topic"
      },
      "source": [
        "We need to separate the pixel values and the label from each other in order for us to load and access it separately. A function was constructed to split the training and testing dataset to separate the labels from the pixel values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "authorized-conjunction",
      "metadata": {
        "id": "authorized-conjunction"
      },
      "outputs": [],
      "source": [
        "def dataframe_to_array(dataframe):\n",
        "    # Make a copy of the original dataframe\n",
        "    dataframe1 = dataframe.copy(deep=True)\n",
        "    # Extract input & outupts as numpy arrays\n",
        "    inputs_array = dataframe1.iloc[:, 1:].to_numpy()\n",
        "    targets_array = dataframe1['label'].to_numpy()\n",
        "    return inputs_array, targets_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "impressed-kingdom",
      "metadata": {
        "id": "impressed-kingdom",
        "outputId": "cdbeae5c-d685-44d3-9194-d9ad5416b1c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(27455, 784) (27455,)\n",
            "(7172, 784) (7172,)\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train = dataframe_to_array(train_dataset)\n",
        "X_test, y_test = dataframe_to_array(test_dataset)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "stupid-onion",
      "metadata": {
        "id": "stupid-onion"
      },
      "source": [
        "Let’s look at the first row of the training dataset. We also need to reshape the array to (28x28) since the initial shape is just a row array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "green-helicopter",
      "metadata": {
        "id": "green-helicopter",
        "outputId": "6221341d-fdca-4457-bb23-30240b6f66f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Letter:  D\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "pic1 = np.reshape(X_train[0], (28, 28))\n",
        "plt.imshow(pic1, cmap = \"gray\")\n",
        "print(\"Letter: \", letters[y_train[0].item()])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "surface-purchase",
      "metadata": {
        "id": "surface-purchase"
      },
      "source": [
        "As expected, the letter in the hand image is D. However, it is evident that the image is not clear due to its small resolution. This may affect the accuracy of our model and the implementation of the model in a much larger scale.\n",
        "\n",
        "The training and testing input arrays are converted to continuous float values since it allows our model for a more precise learning as compared to discrete values. On the other hand, the training and testing labels are converted to long integers since the output of the model are indices to be used in accessing probability values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "terminal-chase",
      "metadata": {
        "id": "terminal-chase"
      },
      "source": [
        "## Q1. Implement `SimpleLogisticRegression` class. (25 points)\n",
        "\n",
        "You are free to search on the internet about implementing a Logistic Regression model using `sklearn.linear_model.LogisticRegression`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "forced-separate",
      "metadata": {
        "id": "forced-separate",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0979bedd-48cf-48ed-d13b-1d144654f7e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "############# Your code here ############\n",
        "\n",
        "#Scale features\n",
        "X_train_norm = X_train/255.\n",
        "X_test_norm = X_test/255.\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "logistic_regression = LogisticRegression(max_iter=1000)\n",
        "logistic_regression.fit(X_train_norm, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = logistic_regression.predict(X_test_norm)\n",
        "\n",
        "#########################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tJsLK3RqSUF",
        "outputId": "68197a57-2022-402a-d7c9-f9db4df70375"
      },
      "id": "-tJsLK3RqSUF",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      1.00      0.92       331\n",
            "           1       1.00      0.91      0.95       432\n",
            "           2       0.86      0.93      0.89       310\n",
            "           3       0.90      0.86      0.88       245\n",
            "           4       0.88      0.89      0.88       498\n",
            "           5       0.72      0.91      0.80       247\n",
            "           6       0.82      0.73      0.78       348\n",
            "           7       0.81      0.70      0.75       436\n",
            "           8       0.55      0.59      0.57       288\n",
            "          10       0.68      0.42      0.52       331\n",
            "          11       0.65      0.89      0.75       209\n",
            "          12       0.71      0.62      0.66       394\n",
            "          13       0.68      0.57      0.62       291\n",
            "          14       0.99      0.62      0.77       246\n",
            "          15       0.93      0.94      0.93       347\n",
            "          16       0.61      0.74      0.67       164\n",
            "          17       0.16      0.43      0.24       144\n",
            "          18       0.33      0.43      0.38       246\n",
            "          19       0.36      0.37      0.37       248\n",
            "          20       0.43      0.46      0.44       266\n",
            "          21       0.88      0.53      0.67       346\n",
            "          22       0.44      0.62      0.51       206\n",
            "          23       0.57      0.46      0.51       267\n",
            "          24       0.70      0.57      0.63       332\n",
            "\n",
            "    accuracy                           0.69      7172\n",
            "   macro avg       0.69      0.68      0.67      7172\n",
            "weighted avg       0.73      0.69      0.70      7172\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "raised-destruction",
      "metadata": {
        "id": "raised-destruction"
      },
      "source": [
        "## Q2. Multi-label classification metrics. (10 points)\n",
        "\n",
        "Search on the web and find **two** best metrics for multi-label classification. Write about how it works and why we use them. Then, implement it using `sklearn.metrics`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Hamming Loss\n",
        "Measures the fraction of labels incorrectly predicted."
      ],
      "metadata": {
        "id": "KygJczsOmsJh"
      },
      "id": "KygJczsOmsJh"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import hamming_loss\n",
        "\n",
        "hamming_loss_value = hamming_loss(y_test, y_pred)\n",
        "print(f\"Hamming Loss: {hamming_loss_value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTZiCxLujyut",
        "outputId": "148b5d45-4cdd-4b96-e79f-b96d527997cf"
      },
      "id": "XTZiCxLujyut",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hamming Loss: 0.3076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Precision, Recall, F1-score (Micro, Macro, Weighted)\n",
        "\n",
        "**Precision** measures the proportion of correctly predicted positive instances (true positives) out of all instances predicted as positive (true positives + false positives).\n",
        "\n",
        "\n",
        "**Recall** measures the proportion of correctly predicted positive instances (true positives) out of all actual positive instances (true positives + false negatives).\n",
        "\n",
        "**F1-score** is the harmonic mean of precision and recall. It balances both precision and recall into a single metric.\n"
      ],
      "metadata": {
        "id": "HNuK9oirmvHX"
      },
      "id": "HNuK9oirmvHX"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "'''\n",
        "Micro\n",
        "Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
        "'''\n",
        "micro_precision = precision_score(y_test, y_pred, average='micro')\n",
        "micro_recall = recall_score(y_test, y_pred, average='micro')\n",
        "micro_f1 = f1_score(y_test, y_pred, average='micro')\n",
        "\n",
        "'''\n",
        "Macro\n",
        "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
        "'''\n",
        "macro_precision = precision_score(y_test, y_pred, average='macro')\n",
        "macro_recall = recall_score(y_test, y_pred, average='macro')\n",
        "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "'''\n",
        "Weighted\n",
        "Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label)\n",
        "This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
        "'''\n",
        "weighted_precision = precision_score(y_test, y_pred, average='weighted')\n",
        "weighted_recall = recall_score(y_test, y_pred, average='weighted')\n",
        "weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Micro Precision: {micro_precision:.4f}, Recall: {micro_recall:.4f}, F1-score: {micro_f1:.4f}\")\n",
        "print(f\"Macro Precision: {macro_precision:.4f}, Recall: {macro_recall:.4f}, F1-score: {macro_f1:.4f}\")\n",
        "print(f\"Weighted Precision: {weighted_precision:.4f}, Recall: {weighted_recall:.4f}, F1-score: {weighted_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdLejvdyj64E",
        "outputId": "a45cc4b9-3256-4bd0-e8b5-0576ee6b7b74"
      },
      "id": "rdLejvdyj64E",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro Precision: 0.6924, Recall: 0.6924, F1-score: 0.6924\n",
            "Macro Precision: 0.6890, Recall: 0.6756, F1-score: 0.6710\n",
            "Weighted Precision: 0.7266, Recall: 0.6924, F1-score: 0.6999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Accuracy\n",
        "\n",
        "Accuracy measures the proportion of correctly predicted labels (both true positives and true negatives) over the total number of labels."
      ],
      "metadata": {
        "id": "5V98_1v0vfjo"
      },
      "id": "5V98_1v0vfjo"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK_tiFe8vfAy",
        "outputId": "47146484-7477-4baa-e10c-911b989633b8"
      },
      "id": "sK_tiFe8vfAy",
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "unlimited-costa",
      "metadata": {
        "id": "unlimited-costa"
      },
      "source": [
        "# 3. Submission\n",
        "\n",
        "Please read the notes here carefully:\n",
        "\n",
        "1. The more beautiful and insightfull your plots and diagrams are, the more points you get. So please take your time and concentration to prepare a good report with nice diagrams.\n",
        "\n",
        "2. The file you upload must be named as `[Student ID]-[Your name].zip` and it must contain **only 1 file**:\n",
        "\n",
        "  - `Linear_and_Logistic_Regression.ipynb`\n",
        "  \n",
        "4. **Important Note**: The outputs of the code blocks must be remained in your notebook, otherwise, you definitly lose all the points of that"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}